%!TEX root = paper.tex
The BitTorrent extension described in the previous section introduces a number of overheads. The introduction of an additional index at each node requires additional local storage space. Discovery of tracking data is achieved by having nodes make requests to other nodes. This requirement increases bandwidth compared with the current BitTorrent protocol. We shall now quantify these overheads.

Bandwidth consumption is calculated using the following assumptions and generalisations about communication costs: 
\begin{enumerate}
    \item TCP/IP overhead amounts to 64 bytes per packet\footnote{\url{http://sd.wareonearth.com/~phil/net/overhead/}}
    \item We never send more bytes than can fit into a single packet (1500 bytes)
    \item BitTorrent requires an initial 68 bytes for a handshake\footnote{\url{http://bittorrent.org/beps/bep_0003.html}}
    \item BitTorrent adds 4 bytes of length prefix per message
    \item A torrent infohash is 20 bytes
    \item Results can be communicated using 6 bytes per node\footnote{Details of the IP and port for each node will make up the results list. \url{http://bittorrent.org/beps/bep_0023.html} explains how BitTorrent clients communicate IP:port combinations in 6 bytes.}
\end{enumerate}

\subsection{Single Node Sending Single Query}

    Using these assumptions we can estimate the communication cost for a node to send a query. For each query the node will contact $z$ other nodes, and receive a response from all of them. An unsuccessful response will contain no details of other nodes. A successful response will contain some number of results. A successful query is one in which at least one response was successful. For a successful query, the total cost depends on, (i) the number of successful requests and, (ii) the number of nodes listed in each response. For simplicity, we consider three cases; responses contain the details of a single node, of ten nodes and of 100 nodes. In practise, the size of a response will vary according to how many applicable nodes each of the queried nodes indexes. The cost, in bytes, to send a query is $z(64+68)=132z$. An unsuccessful response costs $64+4=68$ bytes. A successful response costs $64+4+6a=68+6a$ bytes, where $a$ is the number of results returned, $a=1,10,\textrm{ or }100$. The mean number of successful responses to a query can be determined by taking the expected value of the binomial distribution where each trial (each request) has probability of success $r(t)n^{-1}$. There are $z$ trials (requests) made per query and therefore $zr(t)n^{-1}$ successful responses on average. The expected cost of a query is the combination of the upload cost, $C_u$, to send the requests, and $C_d$, the download cost to receive the responses:

    \begin{align}
        C_u &= 132z\\
        C_d &= 68z(1-r(t)n^{-1}) + (68 + 6a)zr(t)n^{-1}
    \end{align}

    A query will cost a minimum of $132z$ bytes in upload and $68z$ bytes in download. This minimum is seen for unsuccessful queries where every response is empty. In order to estimate the cost of a successful query we need to know $r(t)$. We know that, for constant query rates, Eqn~\ref{eq:rt} tells us that $r(t)$ approaches an asymptote, and, assuming it increases towards this limit, we can derive an upper bound for the download cost:

    \begin{equation}
      \max{C_d} = z(68 + \frac{6au(1+z)}{cn+uz})
    \end{equation}

    Using an example from Section~\ref{sec:extension:discussion}, if $z=100$, $u=100$, $n=5000000$ and $c=0.06$ then we see that the upper bound of the cost to download responses to a query is 6.8Kbytes when $a=1$ and 8.8Kbytes when $a=100$. These costs are acceptable. In fact the size of an average webpage (on 2013/04/15), estimated to be 1.411MB\footnote{\url{http://httparchive.org/interesting.php}}, far exceeds this.

\subsection{Authoring Node Bootstrapping}

    Given a constant query rate, $u$, we know that the replication of a torrent's tracking data approaches a limit at an exponential rate. After a sufficient amount of time the amount of replication in the network will be relatively stable; the new nodes querying for the torrent will increase the replication removed by the nodes that are leaving the network. The replication can either decrease towards this limit or increase, depending on the number of dummy queries performed by the authoring node at time $t=0$. Since the number of replicas in the steady state condition is likely to be too high for a single node to generate we assume the replication to be always increasing towards the steady state limit. The $r(0)$ bootstrap replication then provides a minimum replication level, and so the authoring node can set a minimum probability for successful search. In previous examples we have used $z=100$ and $n=5000000$, if we use these values in Eqn~\ref{eq:prob_find_non_uniform_distribution} we can calculate the required replication for a desired probability of successful query. A minimum probability of $P(d)=0.1$ requires $r(0)=5265$, $P(d)=0.5$ requires $r(0)=34538$. The latter baseline would cost the author $34538*132$ bytes, or 4.5Mbytes. For comparison purposes we ntoe that the default minimum number of packets sent in a ping flood is 100 per second, at this rate the baseline could be reached in just under 6 minutes at an upload bandwidth requirement of 0.1Mbit/s. At this minimum probability a querying node is only expected to have to perform 10 queries before sucess.

\subsection{Single Node Responding to Multiple Queries}

    In order for searches to be successful, queried nodes must respond to requests. In order to estimate the communication cost of providing responses we need to know the number of times a node will be contacted and how large each response will be. In \cite{guo_measurements_2005} the authors observe that the average BitTorrent node will perform $q=1.33$ searches every hour. We can estimate the number of requests generated by the entire network every hour as $qzn$.These requests will be sent to nodes uniformly at random, each individual node receiving an expected $\tfrac{1}{n\text{th}}$ of the total. If we use, as in previous examples, $z=100$, then we would expect each node to receive 133 requests per hour. The cost of responding to these queries depends on the size of the response that can be sent. The longer a node remains in the network the more likely it is to index a requested torrent. If, for simplicity, we assume that for every request received $a=1,10,\textrm{ or }100$ results can be returned, we have that the cost of responding to requests is $133(68 + 6a)$ bytes per hour. The cost of receiving requests is $132*133$ bytes per hour. We estimate that our extension therefore requires 39 bits/sec in download bandwidth and between 22 and 198 bits/sec in upload bandwidth, amounts easily provided, given current home broadband capabilities.

\subsection{Storing the Index}

    In addition to using bandwidth to query and respond, each node must keep a local index of the torrents and nodes it is aware of. If we assume a worst case scenario where each received request is for a distinct torrent, then each item in the index will require 26 bytes; 20 for the infohash and 6 for the requesting node's IP and port details. For a received-request rate, $s$ per hour, and total hours of operation, $h$, the local index size has an upper bound of $26sh$. The local index increases in size at a constant rate. In practise, requests will be received for torrents that are already in the index and so will only require an additional 6 bytes per request. As above, we estimate that nodes will receive, on average, 133 requests per hour. At this rate, a node's local index will reach 1GB after 289,184 hours, or 33 years of continuous use.

    We see that the index size remains comfortably small even after extended usage. We consider then, that the most pressing reason for removing data from the index is to remove incorrect data, i.e. data that suggests that a node owns a torrent when it does not. We briefly consider three strategies for removing incorrect data. Using a Least Recently Used algorithm, nodes could remove old index data to make room for new. Given the churn rate of BitTorrent networks, newer data is more likely to be correct. One of the current BitTorrent DHT implementations periodically sends a ping to indexed nodes. If the ping times out then a negative mark is given that node, too many negative marks and the node is removed. A simpler method would be to remove data from the index as soon as it reaches a certain age. We imagine that a combination of either timed or least recently used with periodic ping will provide the best solution. When a record is considered for deletion, the node is pinged to check for correctness. If the record was in fact correct then it is left in the index. If the record is incorrect it is removed. We leave the analysis of these (and other) solutions as future work.